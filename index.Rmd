---
title: "X is the new Y - contrastive"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document:
    theme: flatly
    highlight: tango
    toc: true
    toc_float: true
    collapsed: false
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Preliminaries

session info:

```{r sessionInfo, message = FALSE, warning = FALSE}

sessionInfo()

```

Install and load packages

```{r pkg, message = FALSE, warning = FALSE}

# install CRAN packages (if not yet installed)
sapply(c("readxl", "tidyverse", "patchwork", "devtools", "vroom", "DT"), function(x) if(!is.element(x, installed.packages())) install.packages(x, dependencies = T))

# install non-CRAN packages (if not yet installed)
if(!is.element("concordances", installed.packages())) {
devtools::install_github("hartmast/concordances")
}

# if this doesn't work, check sfla.ch for the package
if(!is.element("collostructions", installed.packages())) {
  install.packages("https://sfla.ch/wp-content/uploads/2021/02/collostructions_0.2.0.tar.gz", repos = NULL)
}

# load packages
library(readxl)
library(tidyverse)
library(patchwork)
library(vroom)
library(collostructions)
library(DT)

```

## Data wrangling

```{r}

# English
en <- read_xlsx("ENCOW_x_is_the_new_y_without_false_hits.xlsx")

# Spanish
es <- read_xlsx("ESCOW_x_nuev_y_without_false_hits.xlsx")

# German
de <- read_xlsx("DECOW_x_ist_das_neue_y_without_false_hits.xlsx")


```

omit uncertain hits

```{r}
en <- filter(en, keep=="y")
es <- filter(es, keep=="y")
de <- filter(de, keep=="y")

```

## Frequency and productivity measures

Get types, tokens, and hapax legomena for the three languages.

```{r}

# add lowercase lemmas
es$lemma_x <- tolower(es$Lemma_x)
es$lemma_y <- tolower(es$Lemma_y)

de$lemma_x <- tolower(de$Lemma_x)
de$lemma_y <- tolower(de$Lemma_y)

en$lemma_x <- tolower(en$Lemma_x)
en$lemma_y <- tolower(en$Lemma_y)

# add x-y combinations

es$lemma_x_y <- paste0(es$lemma_x, "-", es$lemma_y)
en$lemma_x_y <- paste0(en$lemma_x, "-", en$lemma_y)
de$lemma_x_y <- paste0(de$lemma_x, "-", de$lemma_y)

# get hapaxes --> add frequency counts
en <- en %>% group_by(lemma_x) %>% 
  add_count(name = "count_x") %>%
  ungroup %>% group_by(lemma_y) %>%
  add_count(name = "count_y") %>%
  ungroup %>% group_by(lemma_x_y) %>%
  add_count(name = "count_x_y")

es <- es %>% group_by(lemma_x) %>% 
  add_count(name = "count_x") %>%
  ungroup %>% group_by(lemma_y) %>%
  add_count(name = "count_y") %>%
  ungroup %>% group_by(lemma_x_y) %>%
  add_count(name = "count_x_y")

de <- de %>% group_by(lemma_x) %>% 
  add_count(name = "count_x") %>%
  ungroup %>% group_by(lemma_y) %>%
  add_count(name = "count_y") %>%
  ungroup %>% group_by(lemma_x_y) %>%
  add_count(name = "count_x_y")

# table
tibble(languages = c("English", "German", "Spanish"),
       tokens = c(nrow(en), nrow(de), nrow(es)),
       types_x = c(length(unique(en$lemma_x)),
                   length(unique(de$lemma_x)),
                   length(unique(es$lemma_x))),
       types_y = c(length(unique(en$lemma_y)),
                   length(unique(de$lemma_y)),
                   length(unique(es$lemma_y))),
       types_x_y = c(length(unique(en$lemma_x_y)),
                   length(unique(de$lemma_x_y)),
                   length(unique(es$lemma_x_y))),
       hapaxes_x = c(length(which(en$count_x==1)),
                     length(which(de$count_x==1)),
                     length(which(es$count_x==1))),
       hapaxes_y = c(length(which(en$count_y==1)),
                     length(which(de$count_y==1)),
                     length(which(es$count_y==1))),
       hapaxes_x_y = c(length(which(en$count_x_y==1)),
                     length(which(de$count_x_y==1)),
                     length(which(es$count_x_y==1)))
       
       ) 



```


## Collostructional analysis

For the collostructional analysis, we read in the lemma lists for the English, German, and Spanish data.

```{r, message=FALSE, eval=FALSE}

decow <- vroom("decow16bx.lp.tsv.gz", col_names = c("Lemma", "POS", "Freq"))
escow <- vroom("escow14ax.freq0.l.zip")
encow <- vroom("encow16ax.lp.tsv.gz", col_names = c("Lemma", "POS", "Freq"))

```

Find the lemmas that actually occur in the datasets

```{r, eval=FALSE}

# lemma list ES
es_lemmas <- unique(c(es$Lemma_x, es$Lemma_y))

# lemma list DE
de_lemmas <- unique(c(de$Lemma_x, de$Lemma_y))

# which ESCOW lemmas in es_lemmas?
find_lemmas <- which(escow$token... %in% es_lemmas)
escow <- escow[find_lemmas,]
write_csv(escow, "es_cow_lemmas.csv")

# which DECOW lemmas in de_lemmas?
find_lemmas_de <- which(decow$Lemma %in% de_lemmas)
decow <- decow[find_lemmas_de,]
write_csv(decow, "de_cow_lemmas.csv")


```

Read filtered frequency lists

```{r}

# ESCOW lemma frequencies
escow <- read_csv("es_cow_lemmas.csv")

# DECOW lemma frequencies
decow <- read_csv("de_cow_lemmas.csv")

# ESCOW pos frequencies
es_pos <- read_csv("frequency_lists/ESCOW_pos_frequencies.csv")

# DECOW pos frequencies
de_pos <- read_csv("frequency_lists/decow_pos_frequencies.csv")

# get total number of nouns and adjectives
n_a_freq <- filter(es_pos, pos %in% c("NOUN", "ADP", "PROPN", "ADJ")) %>% select(frequency) %>% sum

n_a_freq_de <- filter(de_pos, tag %in% c("NN", "ADJA", "ADJD", "NE", "NN"))

# collostructional analysis ----


# for the collostructional analysis,
# reduce two-word lemmas to the last word
# (compound heads for compounds, last names
# for person names)
de$lemma_x <- gsub(".* ", "", de$lemma_x)
de$lemma_y <- gsub(".* ", "", de$lemma_y)


# create input for coll. anal. for
# x slot
input_x <- table(es$lemma_x) %>% as_tibble(.name_repair="unique") %>% setNames(c("Lemma", "Freq_in_cxn"))

input_x_de <- table(de$lemma_x) %>% as_tibble(.name_repair="unique") %>% setNames(c("Lemma", "Freq_in_cxn"))

# lowercase lemma in escow
escow$lemma <- tolower(escow$token...)

# lowercase lemma in decow
decow$lemma <- tolower(decow$Lemma)

decow <- decow %>% group_by(lemma) %>% summarise(
  Freq = sum(Freq)
)

# join dataframes to combine lemma and corpus freqs
input_x <- left_join(input_x, select(escow, lemma, f_raw),
          by = c("Lemma" = "lemma"))


input_x_de <- left_join(input_x_de, select(decow, lemma, Freq),
          by = c("Lemma" = "lemma"))

# replace NA by 0
input_x <- input_x %>% replace_na(list(Freq_in_cxn = 0, f_raw = 0))

input_x_de <- input_x_de %>% replace_na(list(Freq_in_cxn = 0, Freq = 0))

# omit all with f_raw < Freq_in_cxn
input_x <- input_x[which(input_x$f_raw>=input_x$Freq_in_cxn),]

input_x_de <- input_x_de[which(input_x_de$Freq >= input_x_de$Freq_in_cxn),]

# same for y -----

# generate input for coll. anal.
input_y_de <- table(de$lemma_y) %>% as_tibble(.name_repair="unique") %>% setNames(c("Lemma", "Freq_in_cxn"))

# join dataframes to combine lemma and corpus freqs
input_y <- left_join(input_y_de, select(escow, lemma, f_raw),
          by = c("Lemma" = "lemma"))


input_y_de <- left_join(input_y_de, select(decow, lemma, Freq),
          by = c("Lemma" = "lemma"))

# replace NA by 0
input_y_de <- input_y_de %>% replace_na(list(Freq_in_cxn = 0, Freq = 0))

input_y_de <- input_y_de[which(input_y_de$Freq >= input_y_de$Freq_in_cxn),]


```

## Simple collexeme analysis - Spanish

```{r}

# coll. anal.
collex_x <- collex(as.data.frame(input_x), corpsize = n_a_freq)


# same for y slot -----

# create input for coll. anal. for
# x slot
input_y <- table(es$lemma_y) %>% as_tibble(.name_repair="unique") %>% setNames(c("Lemma", "Freq_in_cxn"))


input_y <- left_join(input_y, select(escow, lemma, f_raw),
          by = c("Lemma" = "lemma"))

# replace NA by 0
input_y <- input_y %>% replace_na(list(Freq_in_cxn = 0, f_raw = 0))

# omit all with f_raw < Freq_in_cxn
input_y <- input_y[which(input_y$f_raw>=input_y$Freq_in_cxn),]

# coll. anal.
collex_y <- collex(as.data.frame(input_y), corpsize = n_a_freq)


```

Results for x slot

```{r}
collex_x %>% DT::datatable()
```

Results for y slot

```{r}
collex_y %>% DT::datatable()
```

## Simple collexeme analysis - German

```{r}

collex_x_de <- input_x_de %>% as.data.frame %>% collex(corpsize = sum(n_a_freq_de$frequency))

collex_y_de <- input_y_de %>% as.data.frame %>% collex(corpsize = sum(n_a_freq_de$frequency))


```

Results x slot

```{r}

collex_x_de %>% DT::datatable()

```

Results y slot

```{r}

collex_y_de %>% DT::datatable()

```


## Dinstinctive collexeme analysis

```{r}

es %>% ungroup %>% select(lemma_x, lemma_y) %>% as.data.frame() %>% collex.covar() %>% DT::datatable()

de %>% ungroup %>% select(lemma_x, lemma_y) %>% as.data.frame() %>% collex.covar() %>% DT::datatable()

```


## Cross-linguistic distinctive collexeme analysis

For a cross-linguistic distinctive collexeme analysis, we use automatic translations of the German and Spanish lemmas to English generated by ChatGPT.


```{r}

# create input for ChatGPT

# Prompt: "Please translate the following German words to English and return them as a comma-separated list."

# Prompt: "Please translate the following German terms to English. Each row is one term. Please return a table in which the first column contains the original German word and the second column contains the English translation."

# c(de$Lemma_x, de$Lemma_y) %>% unique %>% paste0(collapse = ",")

# Prompt: "Please translate the following Spanish terms to English. Each row is one term. Please return a table in which the first column contains the original Spanish word and the second column contains the English translation."

# c(es$Lemma_x, es$Lemma_y) %>% unique %>% paste0(collapse = ",")


```


Reading in the tables with translations:

```{r}

de_trans <- read_xlsx("cross_linguistic_collexemes/DE_translations.xlsx")
colnames(de_trans) <- c("German", "English")

es_trans <- read_xlsx("cross_linguistic_collexemes/ES_translations.xlsx")


# remove explanatory stuff in brackets:
de_trans$English <- gsub("\\(.*", "", de_trans$English)
es_trans$English <- gsub("\\(.*", "", es_trans$English)

# remove uppercase
de_trans$English <- tolower(de_trans$English)
es_trans$English <- tolower(es_trans$English)

```


Combine with the existing dataframes:

```{r}

# Spanish
es <- left_join(es, es_trans, by = c("Lemma_x" = "Spanish")) 
es <- rename(es, "English_x" = "English")

es <- left_join(es, es_trans, by = c("Lemma_y" = "Spanish")) 
es <- rename(es, "English_y" = "English")


# German
de <- left_join(de, de_trans, by = c("Lemma_x" = "German"))
de <- rename(de, "English_x" = "English")

de <- left_join(de, de_trans, by = c("Lemma_y" = "German"))
de <- rename(de, "English_y" = "English")

```

Distinctive collexeme anlaysis (using collex.covar.mult for multiple distinctive collexeme analysis, see collostructions package vignette)

```{r}


collex_crossling <-  bind_rows(
  tibble(Lemma = de$English_x, Language = "German"),
tibble(Lemma = es$English_x, Language = "Spanish"),
tibble(Lemma = tolower(en$Lemma_x), Language = "English")

) %>% select(Language, Lemma) %>% as.data.frame() %>% collex.covar.mult() 


collex_crossling_y <-  bind_rows(
  tibble(Lemma = de$English_y, Language = "German"),
tibble(Lemma = es$English_y, Language = "Spanish"),
tibble(Lemma = tolower(en$Lemma_y), Language = "English")

) %>% select(Language, Lemma) %>% as.data.frame() %>% collex.covar.mult() 


```

Results x slot English

```{r}

filter(collex_crossling, Language == "English") %>% DT::datatable()

```


Results x slot German

```{r}

filter(collex_crossling, Language == "German") %>% DT::datatable()

```


Results x slot Spanish

```{r}

filter(collex_crossling, Language == "Spanish") %>% DT::datatable()

```

Results x slot English

```{r}

filter(collex_crossling_y, Language == "English") %>% DT::datatable()

```


Results x slot German

```{r}

filter(collex_crossling_y, Language == "German") %>% DT::datatable()

```


Results x slot Spanish

```{r}

filter(collex_crossling_y, Language == "Spanish") %>% DT::datatable()

```




### Combined cross-linguistic distinctive collexeme analysis for x and y slot

```{r}



collex_crossling_combined_input <-  bind_rows(
  tibble(Lemmas = paste0(de$English_x, "-", de$English_y),
       Language = "German"),
tibble(Lemmas = paste0(es$English_x, "-", es$English_y),
       Language = "Spanish"),
tibble(Lemmas = paste0(tolower(en$Lemma_x), "-", tolower(en$Lemma_y)),
       Language = "English")

) 

# manually correct date-oil to data-oil
collex_crossling_combined_input$Lemmas <- gsub("date-oil", "data-oil", collex_crossling_combined_input$Lemmas)

# remove all with NAs
collex_crossling_combined_input <- collex_crossling_combined_input[-grep("NA", collex_crossling_combined_input$Lemmas),]


# collexeme analysis
collex_crossling_combined <- collex_crossling_combined_input %>% select(Language, Lemmas) %>% collex.covar.mult()


```




Results English:

```{r}

collex_crossling_combined %>% filter(Language == "English") %>% mutate(EXP = round(EXP, digits = 2), T = round(T, digits = 2)) %>% DT::datatable()

```

Results German:

```{r}

collex_crossling_combined %>% filter(Language == "German") %>% mutate(EXP = round(EXP, digits = 2), T = round(T, digits = 2)) %>% DT::datatable()

```

Results Spanish:

```{r}

collex_crossling_combined %>% filter(Language == "Spanish") %>% mutate(EXP = round(EXP, digits = 2), T = round(T, digits = 2)) %>% DT::datatable()

```